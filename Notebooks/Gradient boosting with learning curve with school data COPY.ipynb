{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng_th\\Documents\\software\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\ng_th\\Documents\\software\\Anaconda\\lib\\site-packages\\sklearn\\learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\ng_th\\Documents\\software\\Anaconda\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Importing required Python packages \n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse \n",
    "from sklearn import cross_validation \n",
    "from sklearn.cross_validation import ShuffleSplit, train_test_split \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier \n",
    "from sklearn.grid_search import GridSearchCV \n",
    "from sklearn.datasets import load_digits\n",
    "from pprint import pprint \n",
    "\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from pandas.tools.plotting import scatter_matrix \n",
    "import urllib \n",
    "import requests \n",
    "import zipfile \n",
    "from io import StringIO\n",
    "import seaborn \n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "np.random.seed(sum(map(ord, \"aesthetics\"))) \n",
    "seaborn.set_context('notebook') \n",
    "\n",
    "# custom format the graphs  \n",
    "plt.rcParams['figure.figsize'] = (15, 5) \n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Set some Pandas options \n",
    "pd.set_option('display.notebook_repr_html', False) \n",
    "pd.set_option('display.max_columns', 40) \n",
    "pd.set_option('display.max_rows', 25) \n",
    "pd.options.display.max_colwidth = 50 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GradientBooster(param_grid, n_jobs): \n",
    "    estimator = GradientBoostingRegressor() \n",
    "    cv = ShuffleSplit(X_train.shape[1], n_iter=100, test_size=0.2) \n",
    "    classifier = GridSearchCV(estimator=estimator, cv=cv, param_grid=param_grid, n_jobs=n_jobs) \n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(\"Best Estimator learned through GridSearch\")\n",
    "    print(classifier.best_estimator_)\n",
    "    return cv, classifier.best_estimator_ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   zip code  size rank   price  bedroom  Nielson household rank  hotness rank  \\\n",
       "0     66062         57  243300        2                      74           574   \n",
       "1     66062         59  347300        5                      74           574   \n",
       "2     66062         68  277600        4                      74           574   \n",
       "3     66062         69  215500        3                      74           574   \n",
       "4     66061        256  125200        2                     398          5683   \n",
       "\n",
       "   supply score  demand score  median days on market  \n",
       "0      90.94160      95.15714                     29  \n",
       "1      90.94160      95.15714                     29  \n",
       "2      90.94160      95.15714                     29  \n",
       "3      90.94160      95.15714                     29  \n",
       "4      50.86883      73.84731                     58  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv file\n",
    "train = pd.read_csv('../data/zillow_realtor_homes_201804.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>zip code</th>\\n      <th>size rank</th>\\n      <th>price</th>\\n      <th>bedroom</th>\\n      <th>Nielson household rank</th>\\n      <th>hotness rank</th>\\n      <th>supply score</th>\\n      <th>demand score</th>\\n      <th>median days on market</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>count</th>\\n      <td>354.000000</td>\\n      <td>354.000000</td>\\n      <td>3.540000e+02</td>\\n      <td>354.000000</td>\\n      <td>354.000000</td>\\n      <td>354.000000</td>\\n      <td>354.000000</td>\\n      <td>354.000000</td>\\n      <td>354.000000</td>\\n    </tr>\\n    <tr>\\n      <th>mean</th>\\n      <td>64984.943503</td>\\n      <td>4517.031073</td>\\n      <td>2.347398e+05</td>\\n      <td>3.384181</td>\\n      <td>7183.536723</td>\\n      <td>3703.553672</td>\\n      <td>68.560019</td>\\n      <td>79.651767</td>\\n      <td>46.768362</td>\\n    </tr>\\n    <tr>\\n      <th>std</th>\\n      <td>1007.245990</td>\\n      <td>2049.643332</td>\\n      <td>1.375972e+05</td>\\n      <td>1.074757</td>\\n      <td>3471.102686</td>\\n      <td>3103.768103</td>\\n      <td>22.341806</td>\\n      <td>15.862139</td>\\n      <td>21.629707</td>\\n    </tr>\\n    <tr>\\n      <th>min</th>\\n      <td>64012.000000</td>\\n      <td>57.000000</td>\\n      <td>4.790000e+04</td>\\n      <td>1.000000</td>\\n      <td>74.000000</td>\\n      <td>23.000000</td>\\n      <td>10.319300</td>\\n      <td>21.322380</td>\\n      <td>16.000000</td>\\n    </tr>\\n    <tr>\\n      <th>25%</th>\\n      <td>64082.250000</td>\\n      <td>3118.250000</td>\\n      <td>1.390250e+05</td>\\n      <td>3.000000</td>\\n      <td>4801.000000</td>\\n      <td>1280.250000</td>\\n      <td>53.848570</td>\\n      <td>71.507430</td>\\n      <td>31.000000</td>\\n    </tr>\\n    <tr>\\n      <th>50%</th>\\n      <td>64154.000000</td>\\n      <td>4278.500000</td>\\n      <td>2.083000e+05</td>\\n      <td>3.000000</td>\\n      <td>7158.500000</td>\\n      <td>2664.000000</td>\\n      <td>74.559315</td>\\n      <td>83.219375</td>\\n      <td>40.500000</td>\\n    </tr>\\n    <tr>\\n      <th>75%</th>\\n      <td>66106.000000</td>\\n      <td>5957.750000</td>\\n      <td>2.944750e+05</td>\\n      <td>4.000000</td>\\n      <td>9277.000000</td>\\n      <td>5820.000000</td>\\n      <td>88.275520</td>\\n      <td>93.174830</td>\\n      <td>55.000000</td>\\n    </tr>\\n    <tr>\\n      <th>max</th>\\n      <td>66227.000000</td>\\n      <td>10480.000000</td>\\n      <td>1.239500e+06</td>\\n      <td>5.000000</td>\\n      <td>19563.000000</td>\\n      <td>11929.000000</td>\\n      <td>99.046480</td>\\n      <td>99.943540</td>\\n      <td>145.000000</td>\\n    </tr>\\n  </tbody>\\n</table>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe().to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform zip code from category to numpy\n",
    "train[\"zip code\"] = label_encoder.fit_transform(train[\"zip code\"].values) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   zip code  size rank   price  bedroom  Nielson household rank  hotness rank  \\\n",
       "0        71         57  243300        2                      74           574   \n",
       "1        71         59  347300        5                      74           574   \n",
       "2        71         68  277600        4                      74           574   \n",
       "3        71         69  215500        3                      74           574   \n",
       "4        70        256  125200        2                     398          5683   \n",
       "\n",
       "   supply score  demand score  median days on market  \n",
       "0      90.94160      95.15714                     29  \n",
       "1      90.94160      95.15714                     29  \n",
       "2      90.94160      95.15714                     29  \n",
       "3      90.94160      95.15714                     29  \n",
       "4      50.86883      73.84731                     58  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train[\"price\"].values\n",
    "# remove price to get all X values\n",
    "del train[\"price\"]\n",
    "X = train.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid={'n_estimators':[100,500,1000], \n",
    "            'learning_rate': [0.1,0.05,0.02,0.01], \n",
    "            'max_depth':[4,6], \n",
    "            'min_samples_leaf':[3,5,9,17,21], \n",
    "            'max_features':[1.0,0.3,0.1] \n",
    "           }\n",
    "\n",
    "n_jobs=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator learned through GridSearch\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.05, loss='ls', max_depth=4, max_features=0.1,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=5,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "cv,best_est = GradientBooster(param_grid, n_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator Parameters\n",
      "---------------------------\n",
      "n_estimators: 100\n",
      "max_depth: 4\n",
      "Learning Rate: 0.1\n",
      "min_samples_leaf: 5\n",
      "max_features: 0.1\n",
      "Train R-squared: 0.88\n"
     ]
    }
   ],
   "source": [
    "# best estimator parameters\n",
    "print(\"Best Estimator Parameters\")\n",
    "print(\"---------------------------\")\n",
    "print(\"n_estimators: %d\" %best_est.n_estimators)\n",
    "print(\"max_depth: %d\" %best_est.max_depth)\n",
    "print(\"Learning Rate: %.1f\" %best_est.learning_rate)\n",
    "print(\"min_samples_leaf: %d\" %best_est.min_samples_leaf)\n",
    "print(\"max_features: %.1f\" %best_est.max_features)\n",
    "print(\"Train R-squared: %.2f\" %best_est.score(X_train,y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng_th\\Documents\\software\\Anaconda\\lib\\site-packages\\sklearn\\learning_curve.py:235: RuntimeWarning: Removed duplicate entries from 'train_sizes'. Number of ticks will be less than the size of 'train_sizes' 4 instead of 5).\n",
      "  % (train_sizes_abs.shape[0], n_ticks), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "title = \"Learning Curves (Gradient Boosted Regression Trees)\" \n",
    "\n",
    "estimator = GradientBoostingRegressor(n_estimators=best_est.n_estimators, \n",
    "                                      max_depth=best_est.max_depth, \n",
    "                                      learning_rate=best_est.learning_rate, \n",
    "                                      min_samples_leaf=best_est.min_samples_leaf, \n",
    "                                      max_features=best_est.max_features) \n",
    "\n",
    "plot_learning_curve(estimator, title, X_train, y_train, cv=cv, n_jobs=n_jobs) \n",
    "plt.savefig('../image/gbrt_learning_curves_orig.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng_th\\Documents\\software\\Anaconda\\lib\\site-packages\\sklearn\\learning_curve.py:235: RuntimeWarning: Removed duplicate entries from 'train_sizes'. Number of ticks will be less than the size of 'train_sizes' 4 instead of 5).\n",
      "  % (train_sizes_abs.shape[0], n_ticks), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "min_samples_leaf = 25 \n",
    "\n",
    "title = \"Learning Curves (Gradient Boosted Regression Trees), min_samples_leaf=20\" \n",
    "estimator = GradientBoostingRegressor(n_estimators=best_est.n_estimators, \n",
    "                                      max_depth=best_est.max_depth, \n",
    "                                      learning_rate=best_est.learning_rate, \n",
    "                                      min_samples_leaf=min_samples_leaf, \n",
    "                                      max_features=best_est.max_features) \n",
    "\n",
    "plot_learning_curve(estimator, title, X_train, y_train, cv=cv, n_jobs=n_jobs) \n",
    "plt.savefig('../image/gbrt_learning_curves_min_samples_leaf.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng_th\\Documents\\software\\Anaconda\\lib\\site-packages\\sklearn\\learning_curve.py:235: RuntimeWarning: Removed duplicate entries from 'train_sizes'. Number of ticks will be less than the size of 'train_sizes' 4 instead of 5).\n",
      "  % (train_sizes_abs.shape[0], n_ticks), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "max_features = 0.3 \n",
    "\n",
    "title = \"Learning Curves (Gradient Boosted Regression Trees), max_features=50%\" \n",
    "estimator = GradientBoostingRegressor(n_estimators=best_est.n_estimators, \n",
    "                                      max_depth=best_est.max_depth, \n",
    "                                      learning_rate=best_est.learning_rate, \n",
    "                                      min_samples_leaf=min_samples_leaf, \n",
    "                                      max_features=best_est.max_features) \n",
    "\n",
    "plot_learning_curve(estimator, title, X_train, y_train, cv=cv, n_jobs=n_jobs) \n",
    "plt.savefig('../image/gbrt_learning_curves_max_features.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ng_th\\Documents\\software\\Anaconda\\lib\\site-packages\\sklearn\\learning_curve.py:235: RuntimeWarning: Removed duplicate entries from 'train_sizes'. Number of ticks will be less than the size of 'train_sizes' 4 instead of 5).\n",
      "  % (train_sizes_abs.shape[0], n_ticks), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = .01 \n",
    "n_estimators = 1000\n",
    "\n",
    "title = \"Learning Curves (Gradient Boosted Regression Trees), 1000 Trees at learning rate .01\" \n",
    "estimator = GradientBoostingRegressor(n_estimators=best_est.n_estimators, \n",
    "                                      max_depth=best_est.max_depth, \n",
    "                                      learning_rate=best_est.learning_rate, \n",
    "                                      min_samples_leaf=min_samples_leaf, \n",
    "                                      max_features=best_est.max_features) \n",
    "\n",
    "plot_learning_curve(estimator, title, X_train, y_train, cv=cv, n_jobs=n_jobs) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.05, loss='ls', max_depth=4, max_features=0.1,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=25,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 8 important features\n",
      "1. bedroom (0.261003)\n",
      "2. zip code (0.191911)\n",
      "3. demand score (0.117376)\n",
      "4. median days on market (0.106355)\n",
      "5. Nielson household rank (0.099879)\n",
      "6. hotness rank (0.089291)\n",
      "7. size rank (0.075815)\n",
      "8. supply score (0.058371)\n",
      "Mean Feature Importance 0.125000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the feature ranking - Top 8\n",
    "importances = estimator.feature_importances_ \n",
    "indices = np.argsort(importances)[::-1] \n",
    "\n",
    "print(\"Top 8 important features\")\n",
    "for f in range(8): \n",
    "    print(\"%d. %s (%f)\" % (f + 1, train.columns[indices[f]], \n",
    "                                  importances[indices[f]]))\n",
    "\n",
    "print(\"Mean Feature Importance %.6f\" %np.mean(importances))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices=indices[:8] \n",
    "plt.figure() \n",
    "plt.title(\"Top 8 Feature importances\") \n",
    "plt.bar(range(8), importances[indices], color=\"r\", align=\"center\") \n",
    "plt.xticks(range(8), train.columns[indices], fontsize=14, rotation=45) \n",
    "plt.xlim([-1, 8]) \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.05, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = GradientBoostingRegressor(n_estimators=best_est.n_estimators, learning_rate=best_est.learning_rate)\n",
    "estimator.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Estimator Parameters\n",
      "---------------------------\n",
      "n_estimators: 100\n",
      "max_depth: 4\n",
      "Learning Rate: 0.1\n",
      "min_samples_leaf: 5\n",
      "max_features: 0.1\n",
      "\n",
      "Final Train R-squared: 0.88\n",
      "Final Test R-squared: 0.50\n"
     ]
    }
   ],
   "source": [
    "# final estimator parameters\n",
    "print(\"Final Estimator Parameters\")\n",
    "print(\"---------------------------\")\n",
    "print(\"n_estimators: %d\" %best_est.n_estimators)\n",
    "print(\"max_depth: %d\" %best_est.max_depth)\n",
    "print(\"Learning Rate: %.1f\" %best_est.learning_rate)\n",
    "print(\"min_samples_leaf: %d\" %best_est.min_samples_leaf)\n",
    "print(\"max_features: %.1f\" %best_est.max_features)\n",
    "print(\"\")\n",
    "print(\"Final Train R-squared: %.2f\" %best_est.score(X_train,y_train))\n",
    "print(\"Final Test R-squared: %.2f\" %best_est.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict class labels\n",
    "pred = estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.4980\n"
     ]
    }
   ],
   "source": [
    "# score on test data (accuracy)\n",
    "accuracy = best_est.score(X_test, y_test)\n",
    "print('ACC: %.4f' % accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148350.351562\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "Xt = np.matrix([71,57,2,50,500,90,95,20])\n",
    "prediction = estimator.predict(Xt)[0]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[\"zip code\"] = label_encoder.inverse_transform(train[\"zip code\"]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   zip code  size rank  bedroom  Nielson household rank  hotness rank  \\\n",
       "0     66062         57        2                      74           574   \n",
       "1     66062         59        5                      74           574   \n",
       "2     66062         68        4                      74           574   \n",
       "3     66062         69        3                      74           574   \n",
       "4     66061        256        2                     398          5683   \n",
       "\n",
       "   supply score  demand score  median days on market  \n",
       "0      90.94160      95.15714                     29  \n",
       "1      90.94160      95.15714                     29  \n",
       "2      90.94160      95.15714                     29  \n",
       "3      90.94160      95.15714                     29  \n",
       "4      50.86883      73.84731                     58  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/gbtr_model.gbtr']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_est, '../model/gbtr_model.gbtr') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reloaded_model = joblib.load('../model/gbtr_model.gbtr') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203606.893647\n"
     ]
    }
   ],
   "source": [
    "prediction = reloaded_model.predict(Xt)[0]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
